{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "colab": {
      "name": "CB1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lynnlo/CB1/blob/master/CB1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dknxItu1X91"
      },
      "source": [
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.layers import LSTM, BatchNormalization, Embedding, Bidirectional, Dropout, InputLayer, Flatten, Dense, Reshape\n",
        "from matplotlib import pyplot\n",
        "\n",
        "import numpy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmArIztOzstp"
      },
      "source": [
        "# Data\n",
        "from urllib.request import urlopen\n",
        "\n",
        "try:\n",
        "    file = open('local_data.txt').read()\n",
        "except:\n",
        "    file = urlopen(\"https://raw.githubusercontent.com/lynnlo/CB1/master/dialogs.txt\").read().decode('utf-8')\n",
        "    local = open('local_data.txt', 'w')\n",
        "    local.write(file)\n",
        "    local.close()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRBSBMGy0PuH"
      },
      "source": [
        "# Split Data\n",
        "file = file.replace(\"\\n\", \"\\t\")\n",
        "data_raw = file.split(\"\\t\")\n",
        "\n",
        "# Data\n",
        "data = []\n",
        "input, output = [], []\n",
        "dictionary = { '<ov>': 0, ' ': 1 }\n",
        "word_list = ['<ov>', ' ']\n",
        "\n",
        "# Switch for spliting array in groups of 2.\n",
        "t = False\n",
        "\n",
        "# Sizing Params\n",
        "max_size = 0\n",
        "total_data_size = 5000\n",
        "\n",
        "for x in data_raw:\n",
        "    if t:\n",
        "        data[-1].append(x)\n",
        "        t = not t\n",
        "    else:\n",
        "        data.append([x])\n",
        "        t = not t"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi39RMdg3561"
      },
      "source": [
        "for line in data[0:total_data_size]:\n",
        "    # Input\n",
        "    input_sentence = []\n",
        "    for word in line[0].split(' '):\n",
        "        if dictionary.get(word, 0) == 0:\n",
        "            dictionary[word] = len(dictionary)\n",
        "            word_list.append(word)\n",
        "        input_sentence.append(word)\n",
        "    # Update Max Size\n",
        "    if len(input_sentence) > max_size:\n",
        "        max_size = len(input_sentence)\n",
        "    input.append(input_sentence) # Add sentence\n",
        "\n",
        "    # Output\n",
        "    output_sentence = []\n",
        "    for word in line[1].split(' '):\n",
        "        if dictionary.get(word, 0) == 0:\n",
        "            dictionary[word] = len(dictionary)\n",
        "            word_list.append(word)\n",
        "        output_sentence.append(word)\n",
        "    # Update Max Size\n",
        "    if len(input_sentence) > max_size:\n",
        "        max_size = len(input_sentence)\n",
        "    output.append(output_sentence) # Add sentence\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGJp1mQH-J5A"
      },
      "source": [
        "for sentence in input:\n",
        "    while len(sentence) < max_size:\n",
        "        sentence.append(' ')\n",
        "\n",
        "for sentence in output:\n",
        "    while len(sentence) < max_size:\n",
        "        sentence.append(' ')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDg0kPHk-aP7"
      },
      "source": [
        "input_vectorized = []\n",
        "output_vectorized = []\n",
        "\n",
        "for sentence in input:\n",
        "    input_sentence_vectorized = []\n",
        "    for word in sentence:\n",
        "        input_sentence_vectorized.append(dictionary.get(word))\n",
        "    input_vectorized.append(input_sentence_vectorized)\n",
        "\n",
        "for sentence in output:\n",
        "    output_sentence_vectorized = []\n",
        "    for word in sentence:\n",
        "        output_sentence_vectorized.append(dictionary.get(word))\n",
        "    output_vectorized.append(output_sentence_vectorized)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxragoYfraks"
      },
      "source": [
        "input_encoded = []\n",
        "output_encoded = []\n",
        "\n",
        "for x in input_vectorized:\n",
        "    input_encoded.append([])\n",
        "    for i,v in enumerate(x):\n",
        "        ohe = numpy.zeros(len(dictionary))\n",
        "        ohe[v] = 1\n",
        "        input_encoded[-1].append(ohe)\n",
        "\n",
        "for x in output_vectorized:\n",
        "    output_encoded.append([])\n",
        "    for i,v in enumerate(x):\n",
        "        ohe = numpy.zeros(len(dictionary))\n",
        "        ohe[v] = 1\n",
        "        output_encoded[-1].append(ohe)\n",
        "\n",
        "input_encoded = numpy.array(input_encoded)\n",
        "output_encoded = numpy.array(output_encoded)\n",
        "\n",
        "input_encoded = input_encoded.reshape(-1, max_size, len(dictionary))\n",
        "output_encoded = output_encoded.reshape(-1, max_size, len(dictionary))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFZGDuVCTpYm",
        "outputId": "9f723b69-14c8-4ca5-dae9-986226d764e0"
      },
      "source": [
        "input_encoded.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3725, 19, 4042)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHhKD2BDAIJF"
      },
      "source": [
        "model = keras.Sequential()\n",
        "\n",
        "#model.add(Embedding(len(word_list) + 1, len(word_list)))\n",
        "\n",
        "model.add(LSTM(len(word_list), return_sequences=True, input_shape=(max_size, len(word_list))))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gttkYojfgeh"
      },
      "source": [
        "total_history = []\n",
        "model.build(input_encoded.shape)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXIGQAIurUoI"
      },
      "source": [
        "model.fit(x=input_encoded, y=output_encoded, epochs=5, batch_size=64, validation_split=0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKfPw8p51Eq3"
      },
      "source": [
        "total_history = numpy.concatenate((total_history, model.history.history[\"loss\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gYswDwgzkVo"
      },
      "source": [
        "pyplot.plot(total_history)\n",
        "\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVUcfY_fvYeN"
      },
      "source": [
        "def predict(n):\n",
        "    prediction = model.predict(input_encoded[n].reshape(1, max_size, len(word_list))).astype(\"float64\")\n",
        "    prediction_cleaned = []\n",
        "    prediction_readable = []\n",
        "    s = 0\n",
        "    for i in prediction[0][0]:\n",
        "        s += i\n",
        "    print(s)\n",
        "\n",
        "    for x in prediction:\n",
        "        prediction_line = []\n",
        "        for y in x:\n",
        "            prediction_line.append(numpy.argmax(y))\n",
        "        prediction_cleaned.append(prediction_line)\n",
        "\n",
        "    for x in prediction_cleaned[0]:\n",
        "        prediction_readable.append(word_list[x])\n",
        "\n",
        "    print(prediction_cleaned[0])\n",
        "\n",
        "    print(\" \".join(input[n]))\n",
        "    print(\" \".join(output[n]))\n",
        "    print(\" \".join(prediction_readable))\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "for i in range(10):\n",
        "    predict(i)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}